{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Getting and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a spider using Scrapy\n",
    "\n",
    "[Scrapy](https://scrapy.org/) is a powerful tool to scrape web pages.\n",
    "\n",
    "Define a simple spider like the one in [```spider.py```](/edit/spider.py) and then simply run:\n",
    "\n",
    "```\n",
    "$ scrapy runspider spider.py -o dataset-raw.json\n",
    "```\n",
    "\n",
    "This will save all objects created by the spider into a list in the ```dataset-raw.json``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a look at the data\n",
    "\n",
    "An sample article looks like this json object here:\n",
    "```javascript\n",
    "{\n",
    "    \"title\": \"<h1 class=\\\"short \\\" itemprop=\\\"name headline\\\">L\\u2019\\u00e9pouvantail de l\\u2019\\u00abinstabilit\\u00e9\\u00bb</h1>\",\n",
    "    \"body\": \"<div class=\\\"article-main-txt\\\" id=\\\"StoryDetailBody5b15f54930eeaf4a6daf8d8b\\\">\\n        \\n \\n \\n  <p> <strong>Traversons-nous une \\u00abgrande p\\u00e9riode d\\u2019instabilit\\u00e9\\u00bb, comme l\\u2019a martel\\u00e9 Philippe Couillard samedi, au Conseil g\\u00e9n\\u00e9ral lib\\u00e9ral \\u00e0 Montr\\u00e9al?</strong></p> \\n  <p> D\\u2019accord, les perspectives \\u00e9conomiques sont l\\u00e9g\\u00e8rement assombries actuellement.</p> \\n  <p> Notamment par les gestes de protectionnisme du pr\\u00e9sident am\\u00e9ricain Donald Trump, sur l\\u2019acier et l\\u2019aluminium, m\\u00eame envers ses alli\\u00e9s du Canada ...\",\n",
    "    \"keywords\": [\"papier commercial\", \"exag\\u00e9rer la situation\", \"d\\u2019une progression\", \"alli\\u00e9s du Canada\", \"campagne lib\\u00e9rale\", \"contexte \\u00e9conomique\", \"r\\u00e9confortante face\", \"pr\\u00e9sident am\\u00e9ricain\", \"long cycle\", \"budget du ministre\", \"ciel \\u00e9conomique\", \"march\\u00e9 immobilier am\\u00e9ricain\", \"marge du Conseil\", \"adversaire caquiste\", \"r\\u00e9gions du monde\", \"crise financi\\u00e8re\", \"brandir des \\u00e9pouvantails\", \"vrai changement\", \"pr\\u00e9c\\u00e9dant le scrutin\", \"gestes de protectionnisme\", \"premier ministre\", \"promettre des id\\u00e9es\", \"nom du parti\", \"int\\u00e9r\\u00eat \\u00e9lectoral\", \"transformation du Qu\\u00e9bec\", \"vieille formule\", \"grande instabilit\\u00e9\", \"mains sur le volant\", \"Donald Trump\", \"Alexandre Taillefer\", \"Philippe Couillard\", \"Carlos Leitao\", \"M\\u00e9lanie Joly\", \"Lehman Brothers\", \"Montr\\u00e9al\", \"Qu\\u00e9bec\", \"Canada\"],\n",
    "    \"author\": \"Antoine Robitaille\"\n",
    "}\n",
    "```\n",
    "So we need to clean things up a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the data\n",
    "\n",
    "Take a look at the [generate_dataset.py](/edit/generate_dataset.py) file.\n",
    "\n",
    "After running this script (which may take some time depending on the dataset size) we have these objects:\n",
    "\n",
    "```javascript\n",
    "{\n",
    "  \"author\": \"antoine-robitaille\",\n",
    "  \"title\": \"l\\u2019\\u00e9pouvantail_de_l\\u2019\\u00abinstabilit\\u00e9\\u00bb\",\n",
    "  \"keywords\": [\n",
    "   [\"papier\", \"commercial\"],\n",
    "   ...\n",
    "   ],\n",
    "  \"text\": [\n",
    "   [\n",
    "    \"traversons\",\n",
    "    \"-\",\n",
    "    \"nous\",\n",
    "    \"une\",\n",
    "    \"\\u00ab\",\n",
    "    \"grande\",\n",
    "    \"p\\u00e9riode\",\n",
    "    \"d\\u2019\",\n",
    "    \"instabilit\\u00e9\",\n",
    "    \"\\u00bb\",\n",
    "    \",\",\n",
    "    \"comme\",\n",
    "    \"l\\u2019\",\n",
    "    \"a\",\n",
    "    \"martel\\u00e9\",\n",
    "    \"philippe\",\n",
    "    \"couillard\",\n",
    "    \"samedi\",\n",
    "    \",\",\n",
    "    \"au\",\n",
    "    \"conseil\",\n",
    "    ...\n",
    "   ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting our dataset into a train, valid and test set\n",
    "\n",
    "Our dataset is heavily imbalanced by the number of articles per author. We want to reflect this as much as possible in our train, valid and test set.\n",
    "\n",
    "Running the script [```generate_train_valid_test.py```](/edit/generate_train_valid_test.py) will do just that. (With the help of [domain/article.py](/edit/domain/article.py) and [repository/articles.py](/edit/repository/articles.py)).\n",
    "\n",
    "Let's take a look at some exemples;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repository.articles import load_splitted_articles\n",
    "\n",
    "train, valid, test = load_splitted_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training a simple classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our data ready for a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domain.article import stopwords\n",
    "\n",
    "class ArticleSimpleFormatter:\n",
    "    def format_examples(self, articles):\n",
    "        examples = list()\n",
    "        for article in articles:\n",
    "            body = []\n",
    "            for sentence in article.get_sentences():\n",
    "                if len(sentence) > 0:\n",
    "                    body += [w for w in sentence if w not in stopwords]\n",
    "            if len(body) > 0:\n",
    "                examples.append((body[:1000], article.author))\n",
    "        return examples\n",
    "\n",
    "article_formatter = ArticleSimpleFormatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_train = article_formatter.format_examples(train)\n",
    "formatted_valid = article_formatter.format_examples(valid)\n",
    "formatted_test = article_formatter.format_examples(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many authors do we have?\n",
    "len(set(a[1] for a in formatted_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wat is our vocabulary size?\n",
    "len(set([w for a in formatted_train for w in a[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based in [this](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) tutorial, we are going to train a text classifier using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train = [x[0] for x in formatted_train]\n",
    "X_valid = [x[0] for x in formatted_valid]\n",
    "X_test = [x[0] for x in formatted_test]\n",
    "\n",
    "authors = set(x[1] for x in formatted_train)\n",
    "author_to_idx = {author: i for i, author in enumerate(sorted(authors))}\n",
    "\n",
    "Y_train = np.array([author_to_idx[x[1]] for x in formatted_train])\n",
    "Y_valid = np.array([author_to_idx[x[1]] for x in formatted_valid])\n",
    "Y_test = np.array([author_to_idx[x[1]] for x in formatted_test])\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(tokenizer=lambda x: x, lowercase=False)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42,\n",
    "                                           max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, Y_train)\n",
    "predicted = text_clf.predict(X_train)\n",
    "print(np.mean(predicted == Y_train))\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(np.mean(predicted == Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, predicted, target_names=sorted(authors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Getting word vectors from our corpus\n",
    "\n",
    "Let's check what are word vectors and why is it useful. *(see slides)*\n",
    "\n",
    "Now that we are more at ease with word vectors, let's generate them!\n",
    "\n",
    "We first need to format our dataset the way [FastText](https://fasttext.cc/) want it to be by running the [```generate_fasttext_data.py```](/edit/generate_fasttext_data.py)\n",
    "\n",
    "Let's take a look at these word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vec_model_path = './data/ml-quebec-2018.vec'\n",
    "vec_model = KeyedVectors.load_word2vec_format(vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_model['bonjour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector doesn't tell us much about a word itself unless we compare it with other.\n",
    "See [this visualiation](https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/ngarneau/fe6db31a00d99c338eeac5dba7cb32b6/raw/133ee3336c07757f12d85eaf116d82baf48cb56a/config.json) for more insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleFormatter:\n",
    "    def __init__(self, vec_model):\n",
    "        self.vec_model = vec_model\n",
    "\n",
    "    def filter_unfrequent_words(self, sentence):\n",
    "        new_sentence = list()\n",
    "        for word in sentence:\n",
    "            if word in self.vec_model:\n",
    "                new_sentence.append(word)\n",
    "            else:\n",
    "                new_sentence.append('<UNK>')\n",
    "        return new_sentence\n",
    "\n",
    "    def format_examples(self, articles):\n",
    "        examples = list()\n",
    "        for article in articles:\n",
    "            body = []\n",
    "            for sentence in article.get_sentences():\n",
    "                if len(sentence) > 0:\n",
    "                    body += self.filter_unfrequent_words([w for w in sentence if w not in stopwords])\n",
    "            if len(body) > 0:\n",
    "                examples.append((body[:1000], article.author))\n",
    "        return examples\n",
    "\n",
    "article_formatter = ArticleFormatter(vec_model)\n",
    "formatted_train = article_formatter.format_examples(train)\n",
    "formatted_valid = article_formatter.format_examples(valid)\n",
    "formatted_test = article_formatter.format_examples(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "authors = set()\n",
    "\n",
    "for example in formatted_train:\n",
    "    for word in example[0]:\n",
    "        vocab.add(word)\n",
    "    authors.add(example[1])\n",
    "    \n",
    "word_to_idx = {\n",
    "    '<PAD>': 0,\n",
    "    '<UNK>': 1,\n",
    "}\n",
    "\n",
    "for word in sorted(vocab):\n",
    "    word_to_idx[word] = len(word_to_idx)\n",
    "    \n",
    "author_to_idx = {author: i for i, author in enumerate(sorted(authors))}\n",
    "\n",
    "dataset = {\n",
    "    'word_to_idx': word_to_idx,\n",
    "    'author_to_idx': author_to_idx,\n",
    "    'train': formatted_train,\n",
    "    'valid': formatted_valid,\n",
    "    'test': formatted_test,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleVectorizer:\n",
    "    def __init__(self, word_to_idx, author_to_idx):\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.author_to_idx = author_to_idx\n",
    "\n",
    "    def vectorize_sequence(self, sequence, idx, remove_if_unk=False):\n",
    "        if '<UNK>' in idx:\n",
    "            unknown_index = idx['<UNK>']\n",
    "            words = [idx.get(tok, unknown_index) for tok in sequence]\n",
    "            if remove_if_unk:\n",
    "                return [w for w in words if w != unknown_index]\n",
    "            else:\n",
    "                return words\n",
    "\n",
    "        else:\n",
    "            return [idx[tok] for tok in sequence]\n",
    "\n",
    "    def __call__(self, example):\n",
    "        sentences, author = example\n",
    "        vectorized_body = self.vectorize_sequence(sentences, self.word_to_idx)\n",
    "        vectorized_author = self.author_to_idx[author]\n",
    "        return (\n",
    "            vectorized_body,\n",
    "            vectorized_author,\n",
    "        )\n",
    "\n",
    "article_vectorizer = ArticleVectorizer(dataset['word_to_idx'], dataset['author_to_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [article_vectorizer(article) for article in dataset['train']]\n",
    "valid_data = [article_vectorizer(article) for article in dataset['valid']]\n",
    "test_data = [article_vectorizer(article) for article in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentences, author = self.dataset[index]\n",
    "        return sentences, author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentenceDataset(train_data)\n",
    "valid_dataset = SentenceDataset(valid_data)\n",
    "test_dataset = SentenceDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq[:seqlen])\n",
    "    return seq_tensor\n",
    "\n",
    "def collate_truncate(max_length):\n",
    "    def collate_examples(samples):\n",
    "        sentences, authors = list(zip(*samples))\n",
    "        sentences_lengths = torch.LongTensor([min(len(s), max_length) for s in sentences])\n",
    "        padded_sentences = pad_sequences(sentences, sentences_lengths)\n",
    "        authors = torch.LongTensor(authors)\n",
    "        return padded_sentences, authors\n",
    "    return collate_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_truncate(1000),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_truncate(1000),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_truncate(1000),\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_loader))\n",
    "b[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Creating a neural network to classify textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyEmbeddings(nn.Embedding):\n",
    "    def __init__(self, word_to_idx, embedding_dim):\n",
    "        super(MyEmbeddings, self).__init__(len(word_to_idx), embedding_dim, padding_idx=0)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = len(word_to_idx)\n",
    "        self.word_to_idx = word_to_idx\n",
    "\n",
    "    def set_item_embedding(self, idx, embedding):\n",
    "        self.weight.data[idx] = torch.FloatTensor(embedding)\n",
    "\n",
    "    def load_words_embeddings(self, vec_model):\n",
    "        for word in vec_model.index2word:\n",
    "            if word in self.word_to_idx:\n",
    "                idx = self.word_to_idx[word]\n",
    "                embedding = vec_model[word]\n",
    "                self.set_item_embedding(idx, embedding)\n",
    "                \n",
    "embeddings_layer = MyEmbeddings(dataset['word_to_idx'], vec_model.vector_size)\n",
    "embeddings_layer.load_words_embeddings(vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, authors_to_idx, embeddings, conv_in_channels=100, conv_out_channels=256):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.conv = nn.Conv1d(conv_in_channels, conv_out_channels, kernel_size=5, padding=2)\n",
    "        self.fully_connected = nn.Linear(conv_out_channels, len(author_to_idx))\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.metrics = ['acc']\n",
    "\n",
    "    def forward(self, x): # 1\n",
    "        # import pdb; pdb.set_trace()\n",
    "        embeddings = self.embeddings(x) # 2\n",
    "        embeddings = embeddings.transpose(1,2) # 3\n",
    "        convoluted = self.conv(embeddings) # 4\n",
    "        convoluted = F.tanh(convoluted) # 5\n",
    "        pooled = F.max_pool1d(convoluted, convoluted.shape[-1]) # 6\n",
    "        pooled = pooled.squeeze(dim=2) # 7\n",
    "        logits = self.fully_connected(pooled) # 8\n",
    "        return logits\n",
    "\n",
    "\n",
    "loaders = [train_loader, valid_loader, test_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from pytoune.framework import Experiment\n",
    "\n",
    "# Fix bug where memory is allocated on GPU0 when ask to take GPU1.\n",
    "if torch.cuda.is_available() and not args.cpu:\n",
    "    torch.cuda.set_device(args.device)\n",
    "device = torch.device('cuda:%d' % args.device if torch.cuda.is_available() and not args.cpu else 'cpu')\n",
    "\n",
    "# Create our embedding layer\n",
    "embeddings_layer = MyEmbeddings(dataset['word_to_idx'], vec_model.vector_size)\n",
    "# Load pre-trained word embeddings\n",
    "embeddings_layer.load_words_embeddings(vec_model)\n",
    "\n",
    "net = TextClassifier(author_to_idx, embeddings_layer)\n",
    "\n",
    "embeddings_param_set = set(net.embeddings.parameters())\n",
    "other_params_list = [p for p in net.parameters() if p not in embeddings_param_set]\n",
    "optimizer = optim.SGD([{'params': other_params_list, 'lr': 1e-2, 'momentum':0.9, 'weight_decay': 1e-3},\n",
    "                       {'params': net.embeddings.parameters(), 'lr': 1e-3}])\n",
    "\n",
    "expt = Experiment('./expt_random_init', net, optimizer=optimizer, monitor_metric='val_acc', monitor_mode='max', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pytoune.framework import ReduceLROnPlateau\n",
    "\n",
    "def init_and_train(expt,\n",
    "                   loaders,\n",
    "                   callbacks=[],\n",
    "                   reduce_lr_on_plateau=False,\n",
    "                   epochs=2000,\n",
    "                   steps_per_epoch=None,\n",
    "                   logging=True,\n",
    "                   seed=42):\n",
    "    train_loader, valid_loader, test_loader = loaders\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    lr_schedulers = []\n",
    "    if reduce_lr_on_plateau:\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='loss', mode='min', patience=20, factor=0.5, threshold_mode='abs', threshold=1e-3, verbose=True)\n",
    "        lr_schedulers.append(reduce_lr)\n",
    "\n",
    "    expt.train(train_loader, valid_loader,\n",
    "               epochs=epochs,\n",
    "               steps_per_epoch=steps_per_epoch,\n",
    "               validation_steps=steps_per_epoch,\n",
    "               callbacks=callbacks,\n",
    "               lr_schedulers=lr_schedulers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_and_train(expt, loaders, reduce_lr_on_plateau=True, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.test(loaders[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y = list()\n",
    "all_pred = list()\n",
    "\n",
    "for x, y in loaders[2]:\n",
    "    loss, metric, pred = expt.model.evaluate(x, y, return_pred=True)\n",
    "    all_y.append(y)\n",
    "    all_pred.append(pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(np.concatenate(all_y), np.concatenate(all_pred), target_names=sorted(authors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Todos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What about leaving stopwords with a neural net?\n",
    "- The network heavily overfits the data, what can we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
